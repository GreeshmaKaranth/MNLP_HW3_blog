<!DOCTYPE html>
<html lang="en-US" data-theme="light">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-beta.51" />
    <meta name="theme" content="VuePress Theme Hope" />
    <meta property="og:url" content="https://lileicc.github.io/blog/nlp/ERNIE-M/"><meta property="og:site_name" content="MLNLP Blog"><meta property="og:title" content="Understanding ERNIE-M - Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora"><meta property="og:type" content="article"><meta property="og:updated_time" content="2023-11-17T03:12:22.000Z"><meta property="og:locale" content="en-US"><meta property="article:author" content="Anirudh Kannan, Greeshma Karanth"><meta property="article:tag" content="Multilingual NLP"><meta property="article:tag" content="11-737 MNLP Homework 3"><meta property="article:tag" content="Cross-Lingual Semantics"><meta property="article:published_time" content="2023-11-14T00:00:00.000Z"><meta property="article:modified_time" content="2023-11-17T03:12:22.000Z"><title>Understanding ERNIE-M - Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora | MLNLP Blog</title><meta name="description" content="A Blog for Machine Learning, Natural Language Processing, and Data Mining">
    <style>
      :root {
        --bg-color: #fff;
      }

      html[data-theme="dark"] {
        --bg-color: #1d2025;
      }

      html,
      body {
        background-color: var(--bg-color);
      }
    </style>
    <script>
      const userMode = localStorage.getItem("vuepress-theme-hope-scheme");
      const systemDarkMode =
        window.matchMedia &&
        window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (userMode === "dark" || (userMode !== "light" && systemDarkMode)) {
        document.querySelector("html").setAttribute("data-theme", "dark");
      }
    </script>
    <link rel="stylesheet" href="/blog/assets/style.758fbe27.css">
    <link rel="modulepreload" href="/blog/assets/app.addd58cf.js"><link rel="modulepreload" href="/blog/assets/index.html.0163d8e1.js"><link rel="modulepreload" href="/blog/assets/_plugin-vue_export-helper.cdc0426e.js"><link rel="modulepreload" href="/blog/assets/index.html.7f20a3f0.js"><link rel="prefetch" href="/blog/assets/index.html.48d72793.js"><link rel="prefetch" href="/blog/assets/404.html.144ea56c.js"><link rel="prefetch" href="/blog/assets/index.html.3a32b4c3.js"><link rel="prefetch" href="/blog/assets/index.html.3eaba85c.js"><link rel="prefetch" href="/blog/assets/index.html.9c8e527a.js"><link rel="prefetch" href="/blog/assets/index.html.949c6932.js"><link rel="prefetch" href="/blog/assets/index.html.9369be8b.js"><link rel="prefetch" href="/blog/assets/index.html.5ecb22f0.js"><link rel="prefetch" href="/blog/assets/index.html.0a31196e.js"><link rel="prefetch" href="/blog/assets/index.html.b86e98db.js"><link rel="prefetch" href="/blog/assets/index.html.67bd10f9.js"><link rel="prefetch" href="/blog/assets/index.html.05443483.js"><link rel="prefetch" href="/blog/assets/index.html.6acd0970.js"><link rel="prefetch" href="/blog/assets/index.html.61430177.js"><link rel="prefetch" href="/blog/assets/404.html.3600ba0f.js"><link rel="prefetch" href="/blog/assets/index.html.c60b146f.js"><link rel="prefetch" href="/blog/assets/index.html.0a1b4089.js"><link rel="prefetch" href="/blog/assets/index.html.08eb091f.js"><link rel="prefetch" href="/blog/assets/index.html.4d16eeb1.js"><link rel="prefetch" href="/blog/assets/index.html.cd9a0392.js"><link rel="prefetch" href="/blog/assets/index.html.7da6b967.js"><link rel="prefetch" href="/blog/assets/index.html.aa87d896.js"><link rel="prefetch" href="/blog/assets/index.html.63923523.js"><link rel="prefetch" href="/blog/assets/index.html.5c2fa1fd.js"><link rel="prefetch" href="/blog/assets/index.html.06e605b9.js"><link rel="prefetch" href="/blog/assets/index.html.eef3b189.js"><link rel="prefetch" href="/blog/assets/giscus.15440425.js"><link rel="prefetch" href="/blog/assets/highlight.esm.d982e650.js"><link rel="prefetch" href="/blog/assets/markdown.esm.832a189d.js"><link rel="prefetch" href="/blog/assets/math.esm.a3f84b6f.js"><link rel="prefetch" href="/blog/assets/notes.esm.3c361cb7.js"><link rel="prefetch" href="/blog/assets/reveal.esm.b96f05d8.js"><link rel="prefetch" href="/blog/assets/search.esm.80da4a02.js"><link rel="prefetch" href="/blog/assets/zoom.esm.8514a202.js"><link rel="prefetch" href="/blog/assets/photoswipe.esm.382b1873.js">
  </head>
  <body>
    <div id="app"><!--[--><!--[--><!--[--><span tabindex="-1"></span><a href="#main-content" class="skip-link sr-only">Skip to content</a><!--]--><div class="theme-container no-sidebar has-toc"><!--[--><!--[--><header class="navbar"><div class="navbar-left"><button class="toggle-sidebar-button" title="Toggle Sidebar"><span class="icon"></span></button><!----><a href="/blog/" class="brand"><img class="logo" src="/blog/logo.svg" alt="MLNLP Blog"><!----><span class="site-name hide-in-pad">MLNLP Blog</span></a><!----></div><div class="navbar-center"><!----><nav class="nav-links"><div class="nav-item hide-in-mobile"><a href="/blog/" class="nav-link" aria-label="Blog Home"><span class="icon iconfont icon-home"></span>Blog Home<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/category/" class="nav-link" aria-label="Category"><span class="icon iconfont icon-categoryselected"></span>Category<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/tag/" class="nav-link" aria-label="Tags"><span class="icon iconfont icon-tag"></span>Tags<!----></a></div><div class="nav-item hide-in-mobile"><a href="/blog/timeline/" class="nav-link" aria-label="Timeline"><span class="icon iconfont icon-time"></span>Timeline<!----></a></div></nav><!----></div><div class="navbar-right"><!----><!----><div class="nav-item"><a class="repo-link" href="https://github.com/lileicc/blog" target="_blank" rel="noopener noreferrer" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" class="icon github-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="github icon" style="width:1.25rem;height:1.25rem;vertical-align:middle;"><path d="M511.957 21.333C241.024 21.333 21.333 240.981 21.333 512c0 216.832 140.544 400.725 335.574 465.664 24.49 4.395 32.256-10.07 32.256-23.083 0-11.69.256-44.245 0-85.205-136.448 29.61-164.736-64.64-164.736-64.64-22.315-56.704-54.4-71.765-54.4-71.765-44.587-30.464 3.285-29.824 3.285-29.824 49.195 3.413 75.179 50.517 75.179 50.517 43.776 75.008 114.816 53.333 142.762 40.79 4.523-31.66 17.152-53.377 31.19-65.537-108.971-12.458-223.488-54.485-223.488-242.602 0-53.547 19.114-97.323 50.517-131.67-5.035-12.33-21.93-62.293 4.779-129.834 0 0 41.258-13.184 134.912 50.346a469.803 469.803 0 0 1 122.88-16.554c41.642.213 83.626 5.632 122.88 16.554 93.653-63.488 134.784-50.346 134.784-50.346 26.752 67.541 9.898 117.504 4.864 129.834 31.402 34.347 50.474 78.123 50.474 131.67 0 188.586-114.73 230.016-224.042 242.09 17.578 15.232 33.578 44.672 33.578 90.454v135.85c0 13.142 7.936 27.606 32.854 22.87C862.25 912.597 1002.667 728.747 1002.667 512c0-271.019-219.648-490.667-490.71-490.667z"></path></svg></a></div><div class="nav-item hide-in-mobile"><button id="appearance-switch"><svg xmlns="http://www.w3.org/2000/svg" class="icon auto-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="auto icon" style="display:block;"><path d="M512 992C246.92 992 32 777.08 32 512S246.92 32 512 32s480 214.92 480 480-214.92 480-480 480zm0-840c-198.78 0-360 161.22-360 360 0 198.84 161.22 360 360 360s360-161.16 360-360c0-198.78-161.22-360-360-360zm0 660V212c165.72 0 300 134.34 300 300 0 165.72-134.28 300-300 300z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon dark-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="dark icon" style="display:none;"><path d="M524.8 938.667h-4.267a439.893 439.893 0 0 1-313.173-134.4 446.293 446.293 0 0 1-11.093-597.334A432.213 432.213 0 0 1 366.933 90.027a42.667 42.667 0 0 1 45.227 9.386 42.667 42.667 0 0 1 10.24 42.667 358.4 358.4 0 0 0 82.773 375.893 361.387 361.387 0 0 0 376.747 82.774 42.667 42.667 0 0 1 54.187 55.04 433.493 433.493 0 0 1-99.84 154.88 438.613 438.613 0 0 1-311.467 128z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" class="icon light-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="light icon" style="display:none;"><path d="M952 552h-80a40 40 0 0 1 0-80h80a40 40 0 0 1 0 80zM801.88 280.08a41 41 0 0 1-57.96-57.96l57.96-58a41.04 41.04 0 0 1 58 58l-58 57.96zM512 752a240 240 0 1 1 0-480 240 240 0 0 1 0 480zm0-560a40 40 0 0 1-40-40V72a40 40 0 0 1 80 0v80a40 40 0 0 1-40 40zm-289.88 88.08-58-57.96a41.04 41.04 0 0 1 58-58l57.96 58a41 41 0 0 1-57.96 57.96zM192 512a40 40 0 0 1-40 40H72a40 40 0 0 1 0-80h80a40 40 0 0 1 40 40zm30.12 231.92a41 41 0 0 1 57.96 57.96l-57.96 58a41.04 41.04 0 0 1-58-58l58-57.96zM512 832a40 40 0 0 1 40 40v80a40 40 0 0 1-80 0v-80a40 40 0 0 1 40-40zm289.88-88.08 58 57.96a41.04 41.04 0 0 1-58 58l-57.96-58a41 41 0 0 1 57.96-57.96z"></path></svg></button></div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form><!----><button class="toggle-navbar-button" aria-label="Toggle Navbar" aria-expanded="false" aria-controls="nav-screen"><span class="button-container"><span class="button-top"></span><span class="button-middle"></span><span class="button-bottom"></span></span></button></div></header><!----><!--]--><!----><div class="toggle-sidebar-wrapper"><span class="arrow left"></span></div><aside class="sidebar"><!--[--><!----><!--]--><ul class="sidebar-links"></ul><!--[--><!----><!--]--></aside><!--[--><main class="page" id="main-content"><!--[--><!----><nav class="breadcrumb disable"></nav><div class="page-title"><h1><!---->Understanding ERNIE-M - Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora</h1><div class="page-info"><span class="author-info" aria-label="Author🖊" data-balloon-pos="down" localizeddate="November 14, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon author-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="author icon"><path d="M649.6 633.6c86.4-48 147.2-144 147.2-249.6 0-160-128-288-288-288s-288 128-288 288c0 108.8 57.6 201.6 147.2 249.6-121.6 48-214.4 153.6-240 288-3.2 9.6 0 19.2 6.4 25.6 3.2 9.6 12.8 12.8 22.4 12.8h704c9.6 0 19.2-3.2 25.6-12.8 6.4-6.4 9.6-16 6.4-25.6-25.6-134.4-121.6-240-243.2-288z"></path></svg><span><span class="author-item">Anirudh Kannan, Greeshma Karanth</span></span><span property="author" content="Anirudh Kannan, Greeshma Karanth"></span></span><!----><span class="date-info" aria-label="Writing Date📅" data-balloon-pos="down" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon calendar-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="calendar icon"><path d="M716.4 110.137c0-18.753-14.72-33.473-33.472-33.473-18.753 0-33.473 14.72-33.473 33.473v33.473h66.993v-33.473zm-334.87 0c0-18.753-14.72-33.473-33.473-33.473s-33.52 14.72-33.52 33.473v33.473h66.993v-33.473zm468.81 33.52H716.4v100.465c0 18.753-14.72 33.473-33.472 33.473a33.145 33.145 0 01-33.473-33.473V143.657H381.53v100.465c0 18.753-14.72 33.473-33.473 33.473a33.145 33.145 0 01-33.473-33.473V143.657H180.6A134.314 134.314 0 0046.66 277.595v535.756A134.314 134.314 0 00180.6 947.289h669.74a134.36 134.36 0 00133.94-133.938V277.595a134.314 134.314 0 00-133.94-133.938zm33.473 267.877H147.126a33.145 33.145 0 01-33.473-33.473c0-18.752 14.72-33.473 33.473-33.473h736.687c18.752 0 33.472 14.72 33.472 33.473a33.145 33.145 0 01-33.472 33.473z"></path></svg><span>November 14, 2023</span><meta property="datePublished" content="2023-11-14T00:00:00.000Z"></span><span class="category-info" aria-label="Category🌈" data-balloon-pos="down" localizeddate="November 14, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon category-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="category icon"><path d="M148.41 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H148.41c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.311-40.31zM147.556 553.478H429.73c22.263 0 40.311 18.048 40.311 40.31v282.176c0 22.263-18.048 40.312-40.31 40.312H147.555c-22.263 0-40.311-18.049-40.311-40.312V593.79c0-22.263 18.048-40.311 40.31-40.311zM593.927 106.992h282.176c22.263 0 40.31 18.048 40.31 40.31V429.48c0 22.263-18.047 40.31-40.31 40.31H593.927c-22.263 0-40.311-18.047-40.311-40.31V147.302c0-22.263 18.048-40.31 40.31-40.31zM730.22 920.502H623.926c-40.925 0-74.22-33.388-74.22-74.425V623.992c0-41.038 33.387-74.424 74.425-74.424h222.085c41.038 0 74.424 33.226 74.424 74.067v114.233c0 10.244-8.304 18.548-18.547 18.548s-18.548-8.304-18.548-18.548V623.635c0-20.388-16.746-36.974-37.33-36.974H624.13c-20.585 0-37.331 16.747-37.331 37.33v222.086c0 20.585 16.654 37.331 37.126 37.331H730.22c10.243 0 18.547 8.304 18.547 18.547 0 10.244-8.304 18.547-18.547 18.547z"></path></svg><ul class="categories-wrapper"><li class="category category4 clickable" role="navigation">MNLP</li><meta property="articleSection" content="MNLP"></ul></span><span aria-label="Tag🏷" data-balloon-pos="down" localizeddate="November 14, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon tag-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="tag icon"><path d="M939.902 458.563L910.17 144.567c-1.507-16.272-14.465-29.13-30.737-30.737L565.438 84.098h-.402c-3.215 0-5.726 1.005-7.634 2.913l-470.39 470.39a10.004 10.004 0 000 14.164l365.423 365.424c1.909 1.908 4.42 2.913 7.132 2.913s5.223-1.005 7.132-2.913l470.39-470.39c2.01-2.11 3.014-5.023 2.813-8.036zm-240.067-72.121c-35.458 0-64.286-28.828-64.286-64.286s28.828-64.285 64.286-64.285 64.286 28.828 64.286 64.285-28.829 64.286-64.286 64.286z"></path></svg><ul class="tags-wrapper"><li class="tag tag2 clickable" role="navigation">Multilingual NLP</li><li class="tag tag5 clickable" role="navigation">11-737 MNLP Homework 3</li><li class="tag tag8 clickable" role="navigation">Cross-Lingual Semantics</li></ul><meta property="keywords" content="Multilingual NLP,11-737 MNLP Homework 3,Cross-Lingual Semantics"></span><span class="reading-time-info" aria-label="Reading Time⌛" data-balloon-pos="down" localizeddate="November 14, 2023" isoriginal="false" pageview="false"><svg xmlns="http://www.w3.org/2000/svg" class="icon timer-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="timer icon"><path d="M799.387 122.15c4.402-2.978 7.38-7.897 7.38-13.463v-1.165c0-8.933-7.38-16.312-16.312-16.312H256.33c-8.933 0-16.311 7.38-16.311 16.312v1.165c0 5.825 2.977 10.874 7.637 13.592 4.143 194.44 97.22 354.963 220.201 392.763-122.204 37.542-214.893 196.511-220.2 389.397-4.661 5.049-7.638 11.651-7.638 19.03v5.825h566.49v-5.825c0-7.379-2.849-13.981-7.509-18.9-5.049-193.016-97.867-351.985-220.2-389.527 123.24-37.67 216.446-198.453 220.588-392.892zM531.16 450.445v352.632c117.674 1.553 211.787 40.778 211.787 88.676H304.097c0-48.286 95.149-87.382 213.728-88.676V450.445c-93.077-3.107-167.901-81.297-167.901-177.093 0-8.803 6.99-15.793 15.793-15.793 8.803 0 15.794 6.99 15.794 15.793 0 80.261 63.69 145.635 142.01 145.635s142.011-65.374 142.011-145.635c0-8.803 6.99-15.793 15.794-15.793s15.793 6.99 15.793 15.793c0 95.019-73.789 172.82-165.96 177.093z"></path></svg><span>About 9 min</span><meta property="timeRequired" content="PT9M"></span></div><hr></div><div class="toc-place-holder"><aside id="toc"><div class="toc-header">On This Page</div><div class="toc-wrapper"><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#introduction" class="router-link-active router-link-exact-active toc-link level2">Introduction</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#background" class="router-link-active router-link-exact-active toc-link level2">Background</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#ernie-m-methodology" class="router-link-active router-link-exact-active toc-link level2">ERNIE-M Methodology</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#cross-attention-masked-language-modeling-camlm" class="router-link-active router-link-exact-active toc-link level3">Cross-attention Masked Language Modeling (CAMLM)</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#back-translation-masked-language-modeling-btmlm" class="router-link-active router-link-exact-active toc-link level3">Back-translation Masked Language Modeling (BTMLM)</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#experiments-and-results" class="router-link-active router-link-exact-active toc-link level2">Experiments and Results</a></li><ul class="toc-list"><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#ablation-study" class="router-link-active router-link-exact-active toc-link level3">Ablation Study</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#ernie-m-re-implmentation" class="router-link-active router-link-exact-active toc-link level3">ERNIE-M Re-implmentation</a></li><!----><!--]--></ul><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#conclusion" class="router-link-active router-link-exact-active toc-link level2">Conclusion</a></li><!----><!--]--><!--[--><li class="toc-item"><a aria-current="page" href="/blog/nlp/ERNIE-M/#references" class="router-link-active router-link-exact-active toc-link level2">References</a></li><!----><!--]--></ul></div></aside></div><!----><div class="theme-hope-content"><p>Ernie-M: An improved multilingual language model that uses knowledge learned from monolingual corpora to perform cross-lingual tasks better.</p><p>Reading Time: About 10 minutes.</p><!-- more --><p>Paper：<a href="https://aclanthology.org/2021.emnlp-main.3/" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.emnlp-main.3/<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Code: <a href="https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-m" target="_blank" rel="noopener noreferrer">https://github.com/PaddlePaddle/ERNIE/tree/repro/ernie-m<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Demo: <a href="https://huggingface.co/docs/transformers/v4.35.1/en/model_doc/ernie_m#overview" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/v4.35.1/en/model_doc/ernie_m#overview<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h2 id="introduction" tabindex="-1"><a class="header-anchor" href="#introduction" aria-hidden="true">#</a> Introduction</h2><p align="center"><img src="/blog/assets/ernie.67da1fa1.png"></p><p>In the realm of natural language processing, a field reliant on copious data, groundbreaking advancements primarily emerge in high-resource languages like English and Chinese, sidelining thousands of low-resource languages. While the conventional approach involves training individual models for each language, recent years have witnessed substantial progress in multilingual model research. This avenue presents a vision of a future where a single model comprehends all languages, fostering seamless communication and information exchange across diverse linguistic backgrounds.</p><p>Recent research highlights the impressive performance of pre-trained cross-lingual models in downstream tasks, leveraging extensive monolingual and parallel corpora. However, the efficacy of these methods is often hampered by the limited size of parallel corpora, particularly for low-resource languages [1].</p><p>This paper introduces ERNIE-M, a novel multilingual model proficient in understanding 96 languages, coupled with an innovative training method enhancing cross-lingual transferability, even in data-sparse languages. ERNIE-M refers to an enhanced version of the ERNIE (Enhanced Representation through kNowledge Integration) model. This approach encourages the model to align representations across multiple languages with monolingual corpora, mitigating the performance constraints posed by the size of parallel corpora. Experimental results demonstrate ERNIE-M&#39;s superiority over existing cross-lingual models, establishing new benchmarks across various cross-lingual downstream tasks.</p><h2 id="background" tabindex="-1"><a class="header-anchor" href="#background" aria-hidden="true">#</a> Background</h2><p>Contemporary smart systems like search engines, chatbots, and smart speakers rely heavily on extensive labeled data in a single language. Unfortunately, the vast majority of the 6,500 spoken languages globally lack sufficient data, posing a significant challenge for machines to comprehend them, which in turn limits the accessibility of AI.</p><p>Multilingual language models, proficient in understanding and generating text across multiple languages, can be categorized into two main types: discriminative (e.g., mBERT, XLM) and generative (e.g., MASS, mT5). While these models exhibit promise, they encounter limitations in effectively capturing the subtleties of diverse languages. Recent research emphasizes unified multilingual models, showcasing that pretraining cross-lingual language models markedly enhances their performance in cross-lingual natural language processing tasks. These models acquire a language-agnostic representation shared across multiple languages, facilitating transfer learning from well-resourced languages to those with limited resources.</p><p>The conventional effective method involves training a model on various monolingual datasets to grasp semantic representation and align languages on parallel corpora. However, limited sizes of parallel corpora hinder the model&#39;s performance, especially for low-resource languages where parallel data is scarce, making the task more difficult.</p><p>To address these challenges, ERNIE-M introduces innovative approaches inspired by back translation and non-autoregressive neural machine translation. Back translation, known for improving translation performance using parallel data, serves as inspiration for ERNIE-M to enhance its comprehension of different languages. The goal is to overcome previous model limitations and enhance performance in handling diverse languages.</p><h2 id="ernie-m-methodology" tabindex="-1"><a class="header-anchor" href="#ernie-m-methodology" aria-hidden="true">#</a> ERNIE-M Methodology</h2><p>Recent research findings highlight the remarkable success of pre-trained cross-lingual models in excelling at downstream cross-lingual tasks. This achievement stems from the extensive learning acquired from both monolingual and parallel corpora.</p><p>ERNIE-M introduces a key concept: leveraging the transferability gained from parallel corpora to enrich the model&#39;s understanding of large-scale monolingual corpora. This strategy aims to enhance the multilingual semantic representation by fostering the learning of semantic alignments between different languages. The authors implement this by generating pseudo-parallel sentence pairs on a monolingual corpus.</p><p>Based on this idea, the authors propose two pre-training objectives, cross-attention masked language modeling (CAMLM) and back-translation masked language modeling (BTMLM). CAMLM focuses on aligning the cross-lingual semantic representation on parallel corpora. Subsequently, the transferability acquired from parallel corpora is harnessed to elevate the multilingual representation. Specifically, ERNIE-M is trained by using BTMLM, enabling the model to align the semantics of multiple languages from monolingual corpora and enhance its overall multilingual representation.</p><h3 id="cross-attention-masked-language-modeling-camlm" tabindex="-1"><a class="header-anchor" href="#cross-attention-masked-language-modeling-camlm" aria-hidden="true">#</a> Cross-attention Masked Language Modeling (CAMLM)</h3><p>In the first stage of this training process, Cross-attention Masked Language Modeling (CAMLM), the aim is to align cross-lingual semantic representation using a small parallel corpus. Unlike other pre-training methods such as Multilingual Masked Language Modeling (MMLM)[2] and Translation Language Modeling (TLM)[3], which focus on learning a shared language-invariant feature space among multiple languages, CAMLM takes a distinct approach. MMLM implicitly models each language&#39;s semantic representation in a unified feature space, while TLM, an extension of MMLM, captures semantic alignment by learning parallel sentence pairs simultaneously. However, these methods face constraints due to the limited size of available parallel corpora, especially in low-resource languages.</p><p>In contrast, in CAMLM, the multilingual semantic representation is learned by restoring the MASK token in input sentences. When the model restores the MASK token in the source sentence, it can only rely on the semantics of the target sentence. This requires the model to learn how to represent the source language with the semantics of the target sentence, effectively aligning the semantics of multiple languages.</p><p>Unlike TLM, where semantic alignment relies on both source and target sentences, CAMLM only relies on one side of the sentence to restore the MASK token. In TLM, predicting the MASK token is based on the input sentence pair. In CAMLM, the model predicts the MASK token solely based on the sentence of its corresponding parallel sentence and the MASK symbol of this sentence, providing essential position and language information. This unique aspect of CAMLM is advantageous because it avoids information leakage, preventing the model from attending to a pair of input sentences simultaneously. This feature enables the subsequent learning of Back-translation Masked Language Modeling (BTMLM), as illustrated in the figure below.</p><p align="center"><img src="/blog/assets/CAMLM.375576b0.png"></p><p>In this figure, the input sentences in sub-figure (a) are monolingual sentences; x and y represent monolingual input sentences in different languages. The input sentences in sub-figures (b) and (c) are parallel sentences; x and y denote the source and target sentences of the parallel sentences, respectively. h indicates the token predicted by the model.</p><p>Here, a parallel sentence pair is denoted as <code>&lt;source sentence, target sentence&gt;</code>. For example, given a parallel CH-EN sentence pair input as <code>&lt;明天会 [MASK][MASK] 吗，Will it be sunny tomorrow&gt;</code>, the model has to uncover the MASK token <code>&lt;天晴&gt;</code> in the source sentence by solely relying on the meaning of the target sentence, thus learning the semantic representation between the two languages.</p><h3 id="back-translation-masked-language-modeling-btmlm" tabindex="-1"><a class="header-anchor" href="#back-translation-masked-language-modeling-btmlm" aria-hidden="true">#</a> Back-translation Masked Language Modeling (BTMLM)</h3><p>The second stage proposed by the authors is Back-translation Masked Language Modeling (BTMLM), to further train the model, building upon the transferability gained through CAMLM. BTMLM involves generating pseudo-parallel sentences from monolingual sentences, and these generated sentences are then utilized as input for the model to align cross-lingual semantics.</p><p>The learning process for the BTMLM is divided into two stages.</p><ol><li><p><em><strong>Stage 1</strong></em> focuses on generating pseudo-parallel tokens from monolingual corpora. Specifically, the authors insert placeholder MASK tokens at the end of monolingual sentences to indicate the location and language for generation. The model then generates corresponding parallel language tokens based on the original monolingual sentence and the position of the pseudo-token. This process allows the generation of tokens from another language in the monolingual sentence, contributing to the learning of cross-lingual semantic alignment. During the pseudo-token generation, the model can only attend to the source sentence and the placeholder MASK tokens, indicating the language and position to predict, using language embeddings and position embeddings.</p></li><li><p><em><strong>Stage 2</strong></em> utilizes the pseudo-tokens generated in Stage 1 to learn cross-lingual semantics alignment. In the training process of Stage 2, the model&#39;s input is the concatenation of monolingual sentences and generated pseudo-parallel tokens. The learning objective is to restore the MASK tokens based on the original sentences and generated pseudo-parallel tokens. This unique approach allows the model to explicitly learn the alignment of cross-lingual semantic representation from monolingual sentences.</p></li></ol><p>In summary, BTMLM involves predicting part of the tokens in input monolingual sentences into tokens of another language. The predicted tokens and the input sentence are then concatenated as pseudo-parallel sentences to train the model. This two-stage process is illustrated in the figure below, with the left figure representing the first stage of BTMLM, predicting pseudo-tokens, and the right figure representing the second stage, making predictions based on the predicted pseudo-tokens and original sentences.</p><p align="center"><img src="/blog/assets/BMTLM.76a1311c.png" height="300"></p><p>In ERNIE-M, the authors use MMLM and TLM by default because of the strong performance shown in previous work [2][3]. The authors combine MMLM, TLM with CAMLM, BTMLM to train ERNIE-M. The generated pairs from this comprehensive approach serve as input for the model, further aligning cross-lingual semantics and enhancing the multilingual representation. This way, the model can learn sentence alignment with only monolingual corpora and overcome the constraint of the parallel corpus size while improving the model performance.</p><h2 id="experiments-and-results" tabindex="-1"><a class="header-anchor" href="#experiments-and-results" aria-hidden="true">#</a> Experiments and Results</h2><p>The authors examine five cross-lingual evaluation benchmarks, encompassing diverse linguistic tasks. These include XNLI, evaluating cross-lingual natural language inference; MLQA, designed for cross-lingual question answering; CoNLL, specializing in cross-lingual named entity recognition; PAWS-X, tailored for cross-lingual paraphrase identification; and Tatoeba, honing in on cross-lingual retrieval scenarios. These benchmarks collectively serve as a comprehensive framework for evaluating the efficacy of models in a range of cross-lingual applications.</p><p>In the XNLI task, ERNIE-M showcased excellence in both cross-lingual transfer and translate-train-all settings. Outperforming baseline models like XLM, Unicoder, XLM-R, INFOXLM, and VECO, ERNIE-M achieved an average accuracy of 77.3 in cross-lingual transfer and 80.6 in translate-train-all. The larger variant, ERNIE-MLARGE, set a new state-of-the-art with 82.0 accuracy in cross-lingual transfer and 84.2 in translate-train-all, surpassing XLM-RLARGE. The results are summarized in Table 1.</p><p align="center"><img src="/blog/assets/ernie-m-xnli.17288de4.png"></p><p>For NER tasks on CoNLL-2002 and CoNLL-2003, ERNIE-M outshone XLM-R, establishing itself as a state-of-the-art model. Particularly noteworthy was its exceptional performance in low-resource languages, where ERNIE-M surpassed the state-of-the-art in Dutch and German.</p><p align="center"><img src="/blog/assets/ernie-conll.041d2456.png" height="250"></p><p>In the MLQA task for Question Answering, ERNIE-M surpassed various baseline models, achieving a state-of-the-art score with significantly higher F1 and extract match (EM) scores compared to INFOXLM.</p><p align="center"><img src="/blog/assets/ernie-mlqa.8dfe9c5c.png"></p><p>The cross-lingual Paraphrase Identification task, evaluated on the PAWS-X dataset, further demonstrated ERNIE-M&#39;s prowess. It outperformed baseline models in both cross-lingual transfer and translate-train-all settings, showcasing its accuracy and versatility across different languages.</p><p align="center"><img src="/blog/assets/ernie-paws.a8d7a6cd.png"></p><p>Lastly, in the retrieval task using the Tatoeba dataset, ERNIE-M exhibited substantial improvement in accuracy across all languages following fine-tuning with the hardest negative binary cross-entropy loss.</p><p align="center"><img src="/blog/assets/ernie-tatoeba.e7c3b3d5.png" height="200"></p><p>In essence, ERNIE-M emerged as a consistently superior model, surpassing baseline models across a spectrum of cross-lingual tasks. Its effectiveness and robust performance underscore its utility in natural language understanding across diverse linguistic contexts.</p><h3 id="ablation-study" tabindex="-1"><a class="header-anchor" href="#ablation-study" aria-hidden="true">#</a> Ablation Study</h3><p>In the ablation study, a series of experiments (exp0 to exp5) were conducted to understand how aligning semantic representations impacts ERNIE-M&#39;s training.</p><ul><li><p><strong>exp0:</strong> This served as the baseline, involving direct fine-tuning of the XLM-R model on the XNLI and CoNLL tasks.</p></li><li><p><strong>exp1 (MMLM):</strong> ERNIE-M was trained solely on the Masked Language Model (MMLM) using the monolingual corpus to measure the baseline performance gain.</p></li><li><p><strong>exp2 (MMLM + TLM):</strong> This experiment combined MMLM on the monolingual corpus with the Translation Language Model (TLM) on the bilingual corpus to assess the impact of translation-based objectives.</p></li><li><p><strong>exp3 (MMLM + CAMLM):</strong> Integrating MMLM on the monolingual corpus with the Cross-lingual Alignment Model (CAMLM) on the bilingual corpus aimed to evaluate the influence of cross-lingual alignment.</p></li><li><p><strong>exp4 (MMLM + BTMLM + CAMLM):</strong> This experiment incorporated MMLM and Back Translation MMLM (BTMLM) on the monolingual corpus with CAMLM on the bilingual corpus, aiming to combine various training objectives.</p></li><li><p><strong>exp5 (Full Strategy of ERNIE-M):</strong> This represented the complete ERNIE-M strategy, combining multiple training objectives for comprehensive cross-lingual training.</p></li></ul><p align="center"><img src="/blog/assets/ablation.683ef4d8.png" height="200"></p><p>Comparisons between these experiments helped discern the impact of different training objectives on cross-lingual semantic alignment and downstream task performance. Notably, the study revealed that certain combinations, such as BTMLM and CAMLM, proved more effective in capturing cross-lingual semantics, leading to improved performance on tasks like XNLI and CoNLL. The experiments highlighted the importance of selecting appropriate training objectives for achieving superior cross-lingual representation alignment and task-specific outcomes.</p><h3 id="ernie-m-re-implmentation" tabindex="-1"><a class="header-anchor" href="#ernie-m-re-implmentation" aria-hidden="true">#</a> ERNIE-M Re-implmentation</h3><p>We re-implemented both ERNIE and ERNIE-LARGE using Hugging Face and obtained the following results. The evaluations were conducted for the XNLI task across 15 languages.</p><table><thead><tr><th>Model</th><th>ar</th><th>bg</th><th>de</th><th>el</th><th>en</th><th>es</th><th>fr</th><th>hi</th><th>ru</th><th>sw</th><th>th</th><th>tr</th><th>ur</th><th>vi</th><th>zh</th><th>Average</th></tr></thead><tbody><tr><td>ERNIE-M Base</td><td>0.7764</td><td>0.8122</td><td>0.8038</td><td>0.7966</td><td>0.8537</td><td>0.8138</td><td>0.8028</td><td>0.7445</td><td>0.7842</td><td>0.7108</td><td>0.7651</td><td>0.7756</td><td>0.7172</td><td>0.7922</td><td>0.7487</td><td>0.7798</td></tr><tr><td>ERNIE-M Large</td><td>0.8178</td><td>0.8533</td><td>0.8395</td><td>0.8365</td><td>0.8818</td><td>0.8547</td><td>0.8489</td><td>0.7994</td><td>0.8295</td><td>0.7517</td><td>0.8090</td><td>0.8178</td><td>0.7601</td><td>0.8263</td><td>0.7986</td><td>0.8217</td></tr></tbody></table><p>The obtained results align with the anticipated outcomes as documented in the Hugging Face repository, available <a href="https://huggingface.co/MoritzLaurer/ernie-m-large-mnli-xnli" target="_blank" rel="noopener noreferrer">here<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><p>For the implementation details, you can refer to the corresponding Colab notebook, accessible <a href="https://colab.research.google.com/drive/1Mqwyogeq7CupZyILh7a7sFv3Zx2nwngc?usp=sharing" target="_blank" rel="noopener noreferrer">here<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>.</p><h2 id="conclusion" tabindex="-1"><a class="header-anchor" href="#conclusion" aria-hidden="true">#</a> Conclusion</h2><p>To address the limitation imposed by the size of parallel corpora on the performance of cross-lingual models, this paper introduces a novel cross-lingual model, ERNIE-M, trained on both monolingual and parallel corpora. ERNIE-M makes a significant contribution by introducing two key training objectives. The first objective aims to enhance the multilingual representation on parallel corpora through the application of Cross-attention Masked Language Modeling (CAMLM). The second objective focuses on facilitating the alignment of cross-lingual semantic representations from a monolingual corpus using Back-translation Masked Language Modeling (BTMLM). Experimental results demonstrate that ERNIE-M achieves state-of-the-art (SoTA) results across various downstream tasks on datasets such as XNLI, MLQA, CoNLL, PAWS-X, and Tatoeba.</p><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references" aria-hidden="true">#</a> References</h2><p>[1] Xuan Ouyang, Shuohuan Wang, Chao Pang, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. 2021. ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 27–38, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p><p>[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p><p>[3] Alexis Conneau, Guillaume Lample, Ruty Rinott, Adina Williams, Samuel R Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. Xnli: Evaluating cross-lingual sentence representations. arXiv preprint arXiv:1809.05053.</p></div><!----><footer class="page-meta"><div class="meta-item edit-link"><a href="https://github.com/lileicc/blog/edit/main/nlp/ERNIE-M/README.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page" class="nav-link label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" class="icon edit-icon" viewbox="0 0 1024 1024" fill="currentColor" aria-label="edit icon"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></svg><!--]-->Edit this page<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewbox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span><!----></a></div><div class="meta-item update-time"><span class="label">Last update: </span><span class="info">11/17/2023, 3:12:22 AM</span></div><div class="meta-item contributors"><span class="label">Contributors: </span><!--[--><!--[--><span class="contributor" title="email: greeshmakaranth.13@gmail.com">Greeshma Karanth</span>,<!--]--><!--[--><span class="contributor" title="email: anirudh@anirudhs-mbp.mynetworksettings.com">Anirudh Kannan</span><!--]--><!--]--></div></footer><!----><div class="giscus-wrapper input-top" style="display:block;"><div style="text-align:center">Loading...</div></div><!----><!--]--></main><!--]--><footer class="footer-wrapper"><div class="footer">Li Lab</div><div class="copyright">Copyright © 2023 Anirudh Kannan, Greeshma Karanth</div></footer><!--]--></div><!--]--><!----><!--]--></div>
    <script type="module" src="/blog/assets/app.addd58cf.js" defer></script>
  </body>
</html>
